---
layout: post
title: Design Meets Information
---


>*“Overload, clutter, and confusion are not attributes of information, they are failures of design.”*
><div style="text-align:right">– Tufte, from an interview in Carmichael, 2011</div>

By the 20th century, design started to emerge as a stand-alone field. In 1920s Bauhaus established itself as a leading school of thought in design, and continues to have an influence on the field to this day. Bauhaus brought fort a more critical inquiry into the relationship between people and their man-made environment. This school of art was at times criticized for prioritizing form over function, and was considered élitist and alienating by some. Nevertheless, it inspired many great designers, and paved the way for a design as a profession in and of itself.

Yet it was the two World Wars that accelerated technological progress. Countries and organizations pressed for resources have realized the importance of good design, and the benefits of building systems that provide a better human fit. In order to achieve this, the field of industrial designer has emerged. The field gained popularity in part thanks to a celebrity industrial designer Henry Dreyfuss. His work on cars, airplanes, ships, telephones, typewriters, vacuum cleaners, and countless other everyday objects, still surrounds us to this day. Dreyfuss’s *Designing for People* (1955) is considered a classic among industrial designers, and has motivated entire industries to reconsider their approach to design. First and foremost Dreyfuss was a practitioner, primarily concerned with pragmatic aspects of design. Other famous design thinkers include Victor Papanek, R. Buckminster Fuller, and Donald Norman, whose work combines social commentary with foundations of human-centered design.

Industrial design returned the human element into a world blinded by capabilities of the latest technology. It helped transform crude machinery into more enjoyable systems, that have a higher tolerance for human error, and are more sensitive to the context of use. The primary goal of industrial design is to aid the design of physical affordances of the systems we use. This is achieved through the study of human factors and ergonomics. It attempts to answer a question of how to make things more comfortable to use. As industrial design operates in a relatively well structured environment, the utility and purpose of the objects being designed is often defined in advance. For example, a car is used to get one from a point A to a point B, and thus designers do not have to ponder about the purpose of the car. They just need to make the driving experience as pleasant and hassle-free as possible. Information technology has fundamentally changed this relationship between designers and the purpose of the objects they are designing.

Hitherto, many designers would enter the field from an engineering background. An engineering mindset is well suited for structured, well-defined problems, but not as much in the ever-changing, ephemeral digital world. The wide adoption of personal computers and information technology shifted the focus from physical to cognitive aspects of design. The challenge of design shifted from how to sit a person comfortably in front of a machine to a matter of figuring out how to make the machine aid human mental processes. This shift required designers to move away from the compartmentalized physical world, and move into the uncertain realm of human mind, where semantics and pragmatics of information use are not known beforehand (Choo, 2002). Here, the role of a designer is not to simply create an object, but to give it purpose as well. To be able to do so effectively, designers need to build a sense of empathy for the end-user, and familiarize themselves more closely with the context(s) where their designs are being used. Design becomes a much more experimental process in which designers try to uncover the hidden side of human behavior.

In 1990s when the time Internet started to emerge and gain traction with the general public, a field of HCI emerged. The field was a direct answer to an increasingly ubiquitous access to personal computers. Increasingly complex information systems required increasingly complex user interfaces. This new field has introduced a more robust methodology for conducting ethnographic research in relation to design of information systems. It was, however, still rooted in the engineering tradition. As a consequence, solutions were often based on the capabilities of any given system: packing software with every conceivable feature, and stashing away the complexity behind dropdown menus (Cooley, 2000). It followed the mindset of “build it and they will come”, in which the majority of feedback was collected after the design was already completed. This era of design has produced systems that were usable but bloated and vastly over proportionate to the everyday task that they were intended to aid. Assumingly, this was in part a consequence of designers focusing on the organizational information systems and office software. Such designs try to incorporate all of the existing organizational processes into the systems, instead of rethinking those processes from the ground up.

It took another decade and the Dot-com bubble for designers to radically rethink the way they design information systems. More specifically, the launch of the first Apple iPhone in 2007 marked the beginning of a new era in design. While resistive touchscreen technology existed in the past, it was iPhone's capacitive touchscreen that brought this type of interaction with digital interfaces to a mainstream use. It represented a radical shift in the way we interact with the content on the screens around us. It removed the need for navigating with mouse and keyboard, which require a certain level of mental abstraction in order to grasp spatial relations and causality on the screen. Modern touch screens let people interact directly with the content, representing an entirely “different tactile relationship to information” (Andreessen, 2015). This offered designers new possibilities for more immersive experiences, but it also posed a serious constrain, primarily due to much smaller screens and also shorter attention span of the Internet users (Weatherhead, 2014). Suddenly designers were not able to simply hide functionality in endless menus. As a result, services and tools became much more focused, and appear to be more problem-oriented. *Mobile first* design became a new way to conceptualize digital interfaces. This design mindset, proposed by Wroblewski (2011), suggests that every design should start with a bare minimum of the most important content (mobile screen), and gradually layer complexity for larger screens (desktop).

The new category of touch devices renewed interest of designers in information architecture, which has traditionally been a part of LIS. It has also created a whole new job market for user experience (UX), and user interface (UI) designers. These professions existed in the past, but they were considered exotic in a way and only the largest organizations could afford to have such specialized personnel. Now, such designers are a necessity for a successful venture. UX design in particular has significantly improved people participation in the design process, and made ethnographic research a priority for many companies. Unfortunately, it often gets conflated with the UI design (Krishna, 2015), putting the focus again on the interface, and the underlying technology, rather than the person using the system. Also, there is now more tooling available to designers than ever before. According to Papanek (1972) designers can become victims of tyranny of absolute choice because “[w]hen everything becomes possible, when all the limitations are gone, design and art can easily become a never-ending search for novelty, until newness-for-the-sake-of-newness becomes the only measure” (p. 42). This pursuit for newness has been most apparent on the Web, with many designers blindly following the current trends, without ever revisiting the core underlying purpose of their designs.

The field is slowly waking up to a new reality of a never-ending flow of data with no prospect of taming it. There are industry calls for more seamless, less algorithmic designs, and building things that “do not scale” (Krishna, 2015; Graham, 2013). Solutions that do not involve screens for the sake of screens, but choose the medium most appropriate for a given context. The more advanced designs can even predict context(s) and adjust their functionality accordingly. Organizations are also trying to switch their focus from technologically-oriented solutions, to a more problem-based ones, by using Agile and Lean methodologies (Klein, 2013).

These new and evolving approaches to design signal an industry-wide paradigm shift. A design independent of the technology, applied “to the media through which information flows” (Jacobson, 2000, p. 2). Here, designers focus on “the continual development of rich interaction techniques and tools to support user’s ability to create and shape external representations of knowledge that ultimately support more effective situation awareness and understanding” (Pirolli, & Russell, 2011, p. 2). The author of this paper, subsumes these new approaches to design in the term *information design*; a young, ill-defined field with little consistency and consensus across academia.

Information design represents a “philosophical shift of privileging the human's interaction with the information, rather the human's interaction with the computer interface” (Albers, 2008, p. 117). It challenges menu-driven interactions (Cooley, 2000), and calls for a more context-specific information systems (Fidel, 2012). The focus is on edification, the process of personal enlightenment. It is a bottom-up approach in which “information designers seek to edify more than persuade, to exchange ideas rather than foist them on us” (Jacobson, 2000, p. 1). The proposed solution presented later in the paper strives to adhere to these principles of information design.
